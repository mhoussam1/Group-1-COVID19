{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgNG0h/F0dwOEFie33rKMP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhoussam1/Group-1-COVID19/blob/main/practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goaS1psbXIhw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-3.0.3'\n",
        "os.environ['SPARK_VERSION']=spark_version \n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()"
      ],
      "metadata": {
        "id": "CiuC0O7CXhmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ckXqk_wRJ8ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['test.csv']))\n",
        "df2"
      ],
      "metadata": {
        "id": "EhP9KzlEJ8yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5kueO8EXJ82r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HsEVcK-tJ85u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://s3.amazonaws.com/dataviz-curriculum/day_1/food.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"food.csv\"), sep=\",\", header=True,inferSchema=True)"
      ],
      "metadata": {
        "id": "vgPeLxMDfr0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "6bVvQAxSgfO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "OYJ8KW4mregS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "R_weDc-Irg5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "Qeqt6G6mrnm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import struct fields that we can use\n",
        "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
      ],
      "metadata": {
        "id": "YhA5hsTZrqvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next we need to create the list of struct fields\n",
        "schema = [StructField(\"food\", StringType(), True), StructField(\"price\", IntegerType(), True),]\n",
        "schema"
      ],
      "metadata": {
        "id": "wDGerN79r_zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass in our fields\n",
        "final = StructType(fields=schema)\n",
        "final"
      ],
      "metadata": {
        "id": "BuD1FmOAsKoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read our data with our new schema\n",
        "dataframe = spark.read.csv(SparkFiles.get(\"food.csv\"), schema=final, sep=\",\", header=True)\n",
        "dataframe.printSchema()"
      ],
      "metadata": {
        "id": "hcSw1KJOsVgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe['price']"
      ],
      "metadata": {
        "id": "hliQmAsusrOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(dataframe['price'])"
      ],
      "metadata": {
        "id": "_5aoFo7Csx8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.select('price')"
      ],
      "metadata": {
        "id": "bK6BXzqBs33K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(dataframe.select('price'))"
      ],
      "metadata": {
        "id": "IYp6Xzkes-CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.select('price').show()"
      ],
      "metadata": {
        "id": "ECjbmIjJtCPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add new column\n",
        "dataframe.withColumn('newprice', dataframe['price']).show()\n",
        "# Update column name\n",
        "dataframe.withColumnRenamed('price','newerprice').show()\n",
        "# Double the price\n",
        "dataframe.withColumn('doubleprice',dataframe['price']*2).show()\n",
        "# Add a dollar to the price\n",
        "dataframe.withColumn('add_one_dollar',dataframe['price']+1).show()\n",
        "# Half the price\n",
        "dataframe.withColumn('half_price',dataframe['price']/2).show()"
      ],
      "metadata": {
        "id": "1iWFewZWtGca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w311D4qKufSj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}