{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bfbfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a SparkSession\n",
    "import findspark\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType,TimestampType\n",
    "from pyspark.sql import Row\n",
    "import datetime as dt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import *\n",
    "from pyspark.context import SparkContext\n",
    "import pyspark\n",
    "from urllib.request import Request,urlopen\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from botocore.client import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a61c233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create spark session\n",
    "spark = SparkSession.builder.appName(\"project\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accd597d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CountryCode: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- NewConfirmed: long (nullable = true)\n",
      " |-- NewDeaths: long (nullable = true)\n",
      " |-- NewRecovered: long (nullable = true)\n",
      " |-- TotalConfirmed: long (nullable = true)\n",
      " |-- TotalDeaths: long (nullable = true)\n",
      " |-- TotalRecovered: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CountryCode: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- NewConfirmed: integer (nullable = true)\n",
      " |-- NewDeaths: integer (nullable = true)\n",
      " |-- NewRecovered: integer (nullable = true)\n",
      " |-- TotalConfirmed: integer (nullable = true)\n",
      " |-- TotalDeaths: integer (nullable = true)\n",
      " |-- TotalRecovered: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://api.covid19api.com/summary'\n",
    "def get_api_data(url):\n",
    "    ##read table using spark\n",
    "    http=urlopen(url).read().decode('utf-8')\n",
    "    rdd = sc.parallelize([http])\n",
    "    ##pass rdd to json\n",
    "    df_read = spark.read.option(\"multiline\",\"true\")\\\n",
    "             .json(rdd)\n",
    "    ##Explode covid data to select all countries\n",
    "    df_explode = df_read.withColumn('Countries',sf.explode(sf.col('Countries'))).select(['Countries.Country','Countries.CountryCode','Countries.Date','Countries.ID','Countries.NewConfirmed','Countries.NewDeaths','Countries.NewRecovered','Countries.TotalConfirmed','Countries.TotalDeaths','Countries.TotalRecovered'])\n",
    "    df_explode.printSchema()\n",
    "    df_format = df_explode.selectExpr('cast(Country as string) Country',\n",
    "                                      'cast(CountryCode as string) CountryCode',\n",
    "                                      'cast(Date as date) Date',\n",
    "                                      'cast(ID as string) ID',\n",
    "                                      'cast(NewConfirmed as int) NewConfirmed',\n",
    "                                      'cast(NewDeaths as int) NewDeaths',\n",
    "                                      'cast(NewRecovered as int) NewRecovered',\n",
    "                                      'cast(TotalConfirmed as int) TotalConfirmed',\n",
    "                                      'cast(TotalDeaths as int) TotalDeaths',\n",
    "                                      'cast(TotalRecovered as int) TotalRecovered',)\n",
    "    df_format.printSchema()\n",
    "    #Process the data \n",
    "    df_processed =df_format.groupBy('Country','CountryCode','Date','ID','NewConfirmed','NewDeaths','NewRecovered','TotalConfirmed','TotalDeaths','TotalRecovered').count()\n",
    "    #convert pyspark dataframe to pandas\n",
    "    pandas_df = df_processed.toPandas()\n",
    "    print(pandas_df.head())\n",
    "    return pandas_df\n",
    "\n",
    "# ##load transformed dataframe to AWS S3 Bucket\n",
    "ACCESS_KEY_ID =''\n",
    "ACCESS_SECRET_KEY =''\n",
    "BUCKET_NAME = 'project-group1'\n",
    "i = 'covid_dataset.csv'\n",
    "\n",
    "def upload_df(pandas_df,i):\n",
    "    s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY_ID, aws_secret_access_key=ACCESS_SECRET_KEY)\n",
    "    csv_buf=StringIO()\n",
    "    pandas_df.to_csv(csv_buf, header=True, index=False)\n",
    "    csv_buf.seek(0)\n",
    "    s3.put_object(Bucket=BUCKET_NAME, Body=csv_buf.getvalue(), Key='datasets/'+i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pandas_df = get_api_data(url)\n",
    "    #load_s3 = upload_df(pandas_df,i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34152805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd8fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
